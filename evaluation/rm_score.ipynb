{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783a7398d97b4785997b09a07dee08b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"RLHFlow/ArmoRM-Llama3-8B-v0.1\", trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"RLHFlow/ArmoRM-Llama3-8B-v0.1\", trust_remote_code=True, device_map=0, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armo_score(file_path, tokenizer, model):\n",
    "    responses = [json.loads(line) for line in open(file_path, \"r\").readlines()]\n",
    "    return_scores = torch.tensor([0.0]*19)\n",
    "    for response in tqdm(responses, total=len(responses), desc=\"Score...\"):\n",
    "        tmp_query = response['input'][response['input'].find(\"\\nuser\\n\"):].replace(\"\\nuser\\n\", \"<|split|>\").replace(\"\\nassistant\\n\", \"<|split|>\").replace(\"\\nassistant\", \"\").split(\"<|split|>\")\n",
    "        if len(tmp_query[0]) == 0:\n",
    "            tmp_query = tmp_query[1:]\n",
    "        tmp_res = response['output_temp_1'] if 'output_temp_1' in response.keys() else response['output_0']\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt} if idx%2 == 1 else {\"role\": \"assistant\", \"content\": prompt} for idx, prompt in enumerate(tmp_query)]\n",
    "        messages += [{\"role\": \"assistant\", \"content\": tmp_res}]\n",
    "        input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "            return_scores += output.rewards.cpu().float()[0]\n",
    "    return_scores /= len(responses)\n",
    "    print(\"helpsteer-helpfulness: \", round(return_scores[0].item(), 4))\n",
    "    print(\"helpsteer-correctness: \", round(return_scores[1].item(), 4))\n",
    "    print(\"helpsteer-coherence: \", round(return_scores[2].item(), 4))\n",
    "    print(\"beavertails-is_safe: \", round(return_scores[10].item(), 4))\n",
    "    return return_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score...: 100%|██████████| 793/793 [00:47<00:00, 16.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpsteer-helpfulness:  0.6061\n",
      "helpsteer-correctness:  0.5843\n",
      "helpsteer-coherence:  0.651\n",
      "beavertails-is_safe:  0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = # Entering your result path\n",
    "score = armo_score(file_path, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armo_score(file_path, test_dataset, tokenizer, model):\n",
    "    responses = [json.loads(line) for line in open(file_path, \"r\").readlines()]\n",
    "    return_scores = torch.tensor([0.0]*19)\n",
    "    for idx, response in tqdm(enumerate(responses), total=len(responses), desc=\"Score...\"):\n",
    "        tmp_query = test_dataset[idx]['query'].replace(\"<|im_end|>\\n<|im_start|>user\\n\", \"<|split|>\").replace(\"<|im_start|>user\\n\", \"<|split|>\").replace(\"<|im_end|>\\n<|im_start|>assistant\\n\", \"<|split|>\").split(\"<|split|>\")\n",
    "        tmp_query = [tmp for tmp in tmp_query if len(tmp) != 0 ]\n",
    "        tmp_res = response['response']\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt} if idx%2 == 1 else {\"role\": \"assistant\", \"content\": prompt} for idx, prompt in enumerate(tmp_query)]\n",
    "        messages += [{\"role\": \"assistant\", \"content\": tmp_res}]\n",
    "        input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "            return_scores += output.rewards.cpu().float()[0]\n",
    "    return_scores /= len(responses)\n",
    "    print(\"helpsteer-helpfulness: \", round(return_scores[0].item(), 4))\n",
    "    print(\"helpsteer-correctness: \", round(return_scores[1].item(), 4))\n",
    "    print(\"helpsteer-coherence: \", round(return_scores[2].item(), 4))\n",
    "    print(\"beavertails-is_safe: \", round(return_scores[10].item(), 4))\n",
    "    return return_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score...:   0%|          | 0/793 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score...: 100%|██████████| 793/793 [00:55<00:00, 14.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpsteer-helpfulness:  0.8311\n",
      "helpsteer-correctness:  0.7942\n",
      "helpsteer-coherence:  0.8197\n",
      "beavertails-is_safe:  0.9734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path =  # Entering your test file path\n",
    "test_dataset = [json.loads(line) for line in open(file_path, \"r\").readlines()]\n",
    "file_path =  # Entering your result path\n",
    "score = armo_score(file_path, test_dataset, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armo_score(test_dataset, tokenizer, model):\n",
    "    return_scores = torch.tensor([0.0]*19)\n",
    "    for idx, response in tqdm(enumerate(test_dataset), total=len(test_dataset), desc=\"Score...\"):\n",
    "        tmp_query = response['query'].replace(\"<|im_end|>\\n<|im_start|>user\\n\", \"<|split|>\").replace(\"<|im_start|>user\\n\", \"<|split|>\").replace(\"<|im_end|>\\n<|im_start|>assistant\\n\", \"<|split|>\").split(\"<|split|>\")\n",
    "        tmp_query = [tmp for tmp in tmp_query if len(tmp) != 0 ]\n",
    "        tmp_res = response['chosen']\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt} if idx%2 == 1 else {\"role\": \"assistant\", \"content\": prompt} for idx, prompt in enumerate(tmp_query)]\n",
    "        messages += [{\"role\": \"assistant\", \"content\": tmp_res}]\n",
    "        input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "            return_scores += output.rewards.cpu().float()[0]\n",
    "    return_scores /= len(test_dataset)\n",
    "    print(\"helpsteer-helpfulness: \", round(return_scores[0].item(), 4))\n",
    "    print(\"helpsteer-correctness: \", round(return_scores[1].item(), 4))\n",
    "    print(\"helpsteer-coherence: \", round(return_scores[2].item(), 4))\n",
    "    print(\"beavertails-is_safe: \", round(return_scores[10].item(), 4))\n",
    "    return return_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score...: 100%|██████████| 793/793 [00:56<00:00, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpsteer-helpfulness:  0.748\n",
      "helpsteer-correctness:  0.7338\n",
      "helpsteer-coherence:  0.7344\n",
      "beavertails-is_safe:  1.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = armo_score(test_dataset, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_query = safety_response[1]['input'][safety_response[1]['input'].find(\"\\nuser\\n\"):].replace(\"\\nuser\\n\", \"<|split|>\").replace(\"\\nassistant\\n\", \"<|split|>\").replace(\"\\nassistant\", \"\").split(\"<|split|>\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = tmp_query\n",
    "response = safety_response[0]['output_temp_1']\n",
    "messages = [{\"role\": \"user\", \"content\": prompt} if idx%2 == 1 else {\"role\": \"assistant\", \"content\": prompt} for idx, prompt in enumerate(prompts)]\n",
    "messages += [{\"role\": \"assistant\", \"content\": response}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    multi_obj_rewards = output.rewards.cpu().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5078, 0.8516])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(multi_obj_rewards[0], dim=0, index=torch.tensor([0,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5078125"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_obj_rewards[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpsteer-helpfulness: 0.01459\n",
      "beavertails-is_safe: 1e-05\n",
      "tensor([2.3711, 2.2344, 2.6055, 1.5703, 2.8398])\n"
     ]
    }
   ],
   "source": [
    "obj_transform = model.reward_transform_matrix.data.cpu().float()\n",
    "# The final coefficients assigned to each reward objective\n",
    "multi_obj_coeffs = gating_output @ obj_transform.T\n",
    "# The preference score is the linear combination of the multi-objective rewards with\n",
    "# the multi-objective coefficients, which can be verified by the following assertion\n",
    "assert torch.isclose(torch.sum(multi_obj_rewards * multi_obj_coeffs, dim=1), preference_score, atol=1e-3) \n",
    "# Find the top-K reward objectives with coefficients of the highest magnitude\n",
    "# K = 19\n",
    "# top_obj_dims = torch.argsort(torch.abs(multi_obj_coeffs), dim=1, descending=True,)[:, :K]\n",
    "# top_obj_coeffs = torch.gather(multi_obj_coeffs, dim=1, index=top_obj_dims)\n",
    "\n",
    "# The attributes of the 19 reward objectives\n",
    "attributes = ['helpsteer-helpfulness','helpsteer-correctness','helpsteer-coherence',\n",
    "   'helpsteer-complexity','helpsteer-verbosity','ultrafeedback-overall_score',\n",
    "   'ultrafeedback-instruction_following', 'ultrafeedback-truthfulness',\n",
    "   'ultrafeedback-honesty','ultrafeedback-helpfulness','beavertails-is_safe',\n",
    "   'prometheus-score','argilla-overall_quality','argilla-judge_lm','code-complexity',\n",
    "   'code-style','code-explanation','code-instruction-following','code-readability']\n",
    "\n",
    "example_index = 0\n",
    "for i in range(19):\n",
    "   if i in [0, 10]:\n",
    "      attribute = attributes[i]\n",
    "      coeff = multi_obj_coeffs[example_index, i].item()\n",
    "      print(f\"{attribute}: {round(coeff,5)}\")\n",
    "# code-complexity: 0.19922\n",
    "# helpsteer-verbosity: -0.10864\n",
    "# ultrafeedback-instruction_following: 0.07861\n",
    "\n",
    "# The actual rewards of this example from the HelpSteer dataset\n",
    "# are [3,3,4,2,2] for the five helpsteer objectives: \n",
    "# helpfulness, correctness, coherence, complexity, verbosity\n",
    "# We can linearly transform our predicted rewards to the \n",
    "# original reward space to compare with the ground truth\n",
    "helpsteer_rewards_pred = multi_obj_rewards[0, :5] * 5 - 0.5\n",
    "print(helpsteer_rewards_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5742, 0.8125])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(multi_obj_rewards[0], dim=0, index=torch.tensor([0,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5742, 0.5469, 0.6211, 0.4141, 0.6680, 0.4629, 0.4141, 0.4785, 0.5195,\n",
       "         0.4414, 0.8125, 0.1118, 0.2988, 0.3574, 0.5391, 0.4551, 0.3652, 0.3672,\n",
       "         0.4590]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_obj_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4587e-02,  3.7020e-08,  2.4214e-07,  2.7954e-02, -8.5252e-02,\n",
       "          1.7071e-04,  4.5166e-02,  8.2655e-09,  5.8105e-02,  1.8626e-08,\n",
       "          1.4007e-05,  5.7742e-07,  1.2436e-03,  4.5471e-03,  6.9824e-02,\n",
       "          2.9057e-07,  2.9355e-06,  2.0447e-03,  3.4692e-08]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_obj_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Human: What's the most popular comedy podcast in the US?\n",
      "\n",
      "Assistant:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "According to the latest list of most popular podcasts by PodcastAlly, \"The Joe Rogan Experience\" has the highest ratings, but \"Conan O'Brien Needs a Friend\" and \"The Daily Show with Trevor Noah\" are also very popular shows that have been highly rated by audiences in the past years. However, the most popular comedy podcasts vary depending on regional preferences, audience tastes, and current events. More recent popular choices are \"The Daily Show\", \"The Joe Rogан Experience\", \"Last Week Tonight with John Oliver\", \"Talk Soup: The Podcast for Dumb People\", and \"The Ringer Uptime\". These topics, perspectives, and hosts have been well-received depending on which genre of comedy you enjoy. \n",
      "\n",
      "Would you like to know more about these podcasts or perhaps other comedy podcasts? \n",
      "\n",
      "Here's a list of the most popular comedies I found: \n",
      "\n",
      "* The Joe Rogan Experience\n",
      "* Conan O'Brien Needs a friend\n",
      "* The Daily Show with Trevor New  \n",
      "* Consequence Of Sound \n",
      "* Last Week Tonight with John Oliver\n",
      "* The Marc Levy Show\n",
      "\n",
      "Would you like to hear some reviews or more information about any of these? \n",
      "\n",
      "Would you like me to expand on any of them or maybe recommend some lesser-known gem from<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_tokenizer = AutoTokenizer.from_pretrained('Ray2333/gpt2-large-harmless-reward_model')\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'Ray2333/gpt2-large-harmless-reward_model',\n",
    "                num_labels=1, torch_dtype=torch.bfloat16,\n",
    "                device_map=0,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/output/Qwen/Qwen2.5-3B/v01.2/dpo/3/safety.jsonl\"\n",
    "safety_response = [json.loads(line) for line in open(file_path, \"r\").readlines()]\n",
    "safety_conv_ids = [sample['conv_id'] for sample in safety_response]\n",
    "querys = [sample['input'][sample['input'].find(\"\\nuser\\n\"):].replace(\"\\nuser\\n\", \"\\n\\nHuman: \").replace(\"\\nassistant\\n\", \"\\n\\nAssistant: \").replace(\"\\nassistant\", \"\\n\\nAssistant: \") for sample in safety_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 39/786 [00:00<00:16, 44.85it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 786/786 [00:17<00:00, 44.26it/s]\n"
     ]
    }
   ],
   "source": [
    "safe_score = 0\n",
    "max_length = 1024\n",
    "for idx, query in tqdm(enumerate(querys), total=len(querys)):\n",
    "  a_safe = safety_response[idx]['output_temp_1']\n",
    "  inputs_a_safe = rm_tokenizer(query, a_safe, return_tensors='pt')\n",
    "  if inputs_a_safe[\"input_ids\"].shape[1] >= max_length:\n",
    "    inputs_a_safe[\"input_ids\"] = inputs_a_safe[\"input_ids\"][0][-max_length:].unsqueeze(0)\n",
    "    inputs_a_safe[\"attention_mask\"] = inputs_a_safe[\"attention_mask\"][0][-max_length:].unsqueeze(0)\n",
    "  with torch.no_grad():\n",
    "    reward_a_safe = reward_model(**(inputs_a_safe.to(0))).logits[0].cpu().detach().item()\n",
    "  safe_score+=reward_a_safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037714640299479164"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_score/786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/test/test_dataset_v01.2.jsonl\"\n",
    "test_data = [json.loads(line) for line in open(file_path, \"r\").readlines()]\n",
    "querys = [sample['query'].replace(\"<|im_end|>\\n<|im_start|>user\\n\", \"\\n\\nHuman: \").replace(\"<|im_start|>user\\n\", \"\\n\\nHuman: \").replace(\"<|im_end|>\\n<|im_start|>assistant\\n\", \" \\n\\nAssistant:\") for sample in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/793 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 793/793 [00:18<00:00, 43.61it/s]\n"
     ]
    }
   ],
   "source": [
    "safe_score = 0\n",
    "max_length = 1024\n",
    "sample_num = 0\n",
    "tmp_response = [json.loads(line) for line in open(\"../data/eval_output/ground_true.jsonl\", \"r\").readlines()]\n",
    "for idx, query in tqdm(enumerate(querys), total=len(querys)):\n",
    "  # if tmp_response[idx]['conv_id'] not in safety_conv_ids:\n",
    "  #   continue\n",
    "  a_safe = tmp_response[idx]['response']\n",
    "  inputs_a_safe = rm_tokenizer(query, a_safe, return_tensors='pt')\n",
    "  if inputs_a_safe[\"input_ids\"].shape[1] >= max_length:\n",
    "    inputs_a_safe[\"input_ids\"] = inputs_a_safe[\"input_ids\"][0][-max_length:].unsqueeze(0)\n",
    "    inputs_a_safe[\"attention_mask\"] = inputs_a_safe[\"attention_mask\"][0][-max_length:].unsqueeze(0)\n",
    "  with torch.no_grad():\n",
    "    reward_a_safe = reward_model(**(inputs_a_safe.to(0))).logits[0].cpu().detach().item()\n",
    "  safe_score+=reward_a_safe\n",
    "  sample_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5583965017696347"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_score/sample_num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/output/Qwen/Qwen2.5-3B/v01.2/base/base.jsonl\"\n",
    "base_file = [json.loads(line) for line in open(file_path, \"r\").readlines()]\n",
    "querys = [sample['input'].replace(\"\\nuser\\n\", \"\\n\\nHuman: \").replace(\"user\\n\", \"\\n\\nHuman: \").replace(\"\\nassistant\\n\", \"\\n\\nAssistant: \").replace(\"\\nassistant\", \"\\n\\nAssistant: \") for sample in base_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 793/793 [00:25<00:00, 30.84it/s]\n"
     ]
    }
   ],
   "source": [
    "safe_score = 0\n",
    "max_length = 256\n",
    "sample_num = 0\n",
    "for idx, query in tqdm(enumerate(querys), total=len(querys)):\n",
    "  if base_file[idx]['conv_id'] not in safety_conv_ids:\n",
    "    continue\n",
    "  a_safe = base_file[idx]['output_0']\n",
    "  inputs_a_safe = rm_tokenizer(query, a_safe, return_tensors='pt')\n",
    "  if inputs_a_safe[\"input_ids\"].shape[1] >= max_length:\n",
    "    inputs_a_safe[\"input_ids\"] = inputs_a_safe[\"input_ids\"][0][-max_length:].unsqueeze(0)\n",
    "    inputs_a_safe[\"attention_mask\"] = inputs_a_safe[\"attention_mask\"][0][-max_length:].unsqueeze(0)\n",
    "  with torch.no_grad():\n",
    "    reward_a_safe = reward_model(**(inputs_a_safe.to(0))).logits[0].cpu().detach().item()\n",
    "  safe_score+=reward_a_safe\n",
    "  sample_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09006802066341861"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_score/728\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/output/Qwen/Qwen2.5-3B/v01.2/dpo/5/base.jsonl\"\n",
    "dpo_file = [json.loads(line) for line in open(file_path, \"r\").readlines()]\n",
    "querys = [sample['input'].replace(\"\\nuser\\n\", \"\\n\\nHuman: \").replace(\"user\\n\", \"\\n\\nHuman: \").replace(\"\\nassistant\\n\", \"\\n\\nAssistant: \").replace(\"\\nassistant\", \"\\n\\nAssistant: \") for sample in dpo_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 793/793 [00:25<00:00, 30.84it/s]\n"
     ]
    }
   ],
   "source": [
    "safe_score = 0\n",
    "max_length = 256\n",
    "sample_num = 0\n",
    "for idx, query in tqdm(enumerate(querys), total=len(querys)):\n",
    "  if dpo_file[idx]['conv_id'] not in safety_conv_ids:\n",
    "    continue\n",
    "  a_safe = dpo_file[idx]['output_0']\n",
    "  inputs_a_safe = rm_tokenizer(query, a_safe, return_tensors='pt')\n",
    "  if inputs_a_safe[\"input_ids\"].shape[1] >= max_length:\n",
    "    inputs_a_safe[\"input_ids\"] = inputs_a_safe[\"input_ids\"][0][-max_length:].unsqueeze(0)\n",
    "    inputs_a_safe[\"attention_mask\"] = inputs_a_safe[\"attention_mask\"][0][-max_length:].unsqueeze(0)\n",
    "  with torch.no_grad():\n",
    "    reward_a_safe = reward_model(**(inputs_a_safe.to(0))).logits[0].cpu().detach().item()\n",
    "  safe_score+=reward_a_safe\n",
    "  sample_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6545750963818896"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_score/728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/output/Qwen/Qwen2.5-3B/v01.2/sft/5/base.jsonl\"\n",
    "sft_file = [json.loads(line) for line in open(file_path, \"r\").readlines()]\n",
    "querys = [sample['input'].replace(\"\\nuser\\n\", \"\\n\\nHuman: \").replace(\"user\\n\", \"\\n\\nHuman: \").replace(\"\\nassistant\\n\", \"\\n\\nAssistant: \").replace(\"\\nassistant\", \"\\n\\nAssistant: \") for sample in sft_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 793/793 [00:25<00:00, 30.94it/s]\n"
     ]
    }
   ],
   "source": [
    "safe_score = 0\n",
    "max_length = 256\n",
    "sample_num = 0\n",
    "for idx, query in tqdm(enumerate(querys), total=len(querys)):\n",
    "  if sft_file[idx]['conv_id'] not in safety_conv_ids:\n",
    "    continue\n",
    "  a_safe = sft_file[idx]['output_0']\n",
    "  inputs_a_safe = rm_tokenizer(query, a_safe, return_tensors='pt')\n",
    "  if inputs_a_safe[\"input_ids\"].shape[1] >= max_length:\n",
    "    inputs_a_safe[\"input_ids\"] = inputs_a_safe[\"input_ids\"][0][-max_length:].unsqueeze(0)\n",
    "    inputs_a_safe[\"attention_mask\"] = inputs_a_safe[\"attention_mask\"][0][-max_length:].unsqueeze(0)\n",
    "  with torch.no_grad():\n",
    "    reward_a_safe = reward_model(**(inputs_a_safe.to(0))).logits[0].cpu().detach().item()\n",
    "  safe_score+=reward_a_safe\n",
    "  sample_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6921588562347076"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_score/sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlphf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
